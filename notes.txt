#    .set("spark.cassandra.connection.host", CASSANDRA_PORT)
# CASSANDRA_PORT = "172.31.2.11"


command line terminals to start running file in Spark Cluster

pwd
cd ..
cd ..
cd usr/local/spark/
pwd
ls -la (this lists all of the file name)
./bin/pyspark (produces interactive shell with Python and spark)

(need this to start running python code on spark cluster):
ubuntu@ip-172-31-2-9:/usr/local/spark$ ./bin/spark-submit <path to your python file>


#need this to run code in ubuntu:
#./bin/spark-submit examples/s3Setup.py


# rdd = f.map(lambda line: [x for x in line.split('\t')])
# df = rdd.toDF(['GLOBALEVENTID','SQLDATE','MonthYear',
#                'Year','FractionDate','Actor1Code',
#                'Actor1Name','Actor1CountryCode', 'Actor1KnownGroupCode',
#                'Actor1EthnicCode','Actor1Religion1Code','Actor1Religion2Code',
#                'Actor1Type1Code','Actor1Type2Code','Actor1Type3Code',
#                'Actor2Code','Actor2Name','Actor2CountryCode',
#                'Actor2KnownGroupCode','Actor2EthnicCode','Actor2Religion1Code',
#                'Actor2Religion2Code','Actor2Type1Code','Actor2Type2Code',
#                'Actor2Type3Code','IsRootEvent','EventCode',
#                'EventBaseCode','EventRootCode','QuadClass',
#                'GoldsteinScale','NumMentions','NumSources',
#                'NumArticles','AvgTone','Actor1Geo_Type',
#                'Actor1Geo_FullName','Actor1Geo_CountryCode','Actor1Geo_ADM1Code',
#                'Actor1Geo_Lat','Actor1Geo_Long','Actor1Geo_FeatureID',
#                'Actor2Geo_Type','Actor2Geo_FullName','Actor2Geo_CountryCode',
#                'Actor2Geo_ADM1Code','Actor2Geo_Lat','Actor2Geo_Long',
#                'Actor2Geo_FeatureID','ActionGeo_Type','ActionGeo_FullName',
#                'ActionGeo_CountryCode','ActionGeo_ADM1Code','ActionGeo_Lat',
#                'ActionGeo_Long','ActionGeo_FeatureID','DATEADDED','SOURCEURL'])


nmon linux might want to use


http://stackoverflow.com/questions/33417341/connecting-integrating-cassandra-with-spark-pyspark
